import os
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.tensorboard import SummaryWriter
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import random
import numpy as np

# === å¼•å…¥æ¨¡å— ===
from src.config import Config
from src.data.data_loader import build_dataloader
from src.model.model import Text2GestureModel 
from src.model.diffusion import GaussianDiffusion




# print("\n" + "="*50)
# print("ğŸ” ä¾¦æ¢æ¨¡å¼ï¼šæˆ‘æ­£åœ¨è¯»å–çš„æ•°æ®è·¯å¾„æ˜¯ï¼š")
# # è¿™é‡Œé€šå¸¸æ˜¯ args.train_data_path æˆ–è€… Config.train_data_path
# # å¦‚æœä½ ä¸ç¡®å®šå˜é‡åï¼Œå¯ä»¥æœä¸€ä¸‹ä»£ç é‡Œ DataPreprocessor è¢«è°ƒç”¨çš„åœ°æ–¹
# try:
#     print(f"PATH: {args.train_data_path}") 
# except:
#     try:
#         print(f"PATH: {Config.train_data_path}") # æˆ–è€…ç±»ä¼¼çš„å˜é‡å
#     except:
#         print("æ— æ³•è‡ªåŠ¨æ‰¾åˆ°å˜é‡ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥ä»£ç ä¸­ Dataset åˆå§‹åŒ–çš„ä½ç½®")
# print("="*50 + "\n")


# exit(0)

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def evaluate(diffusion, val_loader, text_encoder, device):
    """
    éªŒè¯å‡½æ•°ï¼šè®¡ç®—æµ‹è¯•é›†ä¸Šçš„å¹³å‡ Loss
    """
    diffusion.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    total_loss = 0
    
    with torch.no_grad(): # ä¸è®¡ç®—æ¢¯åº¦
        for batch in val_loader:
            # 1. å‡†å¤‡æ•°æ®
            motions = batch['x'].to(device)
            cond_ids = batch['cond'].to(device)
            cond_mask = batch['cond_mask'].to(device)

            # 2. æ–‡æœ¬ç¼–ç 
            text_outputs = text_encoder(input_ids=cond_ids, attention_mask=cond_mask)
            text_embeddings = text_outputs.last_hidden_state

            # 3. è®¡ç®— Diffusion Loss (éªŒè¯æ—¶ä¸åŠ  CFG Maskï¼Œå› ä¸ºæˆ‘ä»¬è¦çœ‹çœŸå®çš„ç”Ÿæˆèƒ½åŠ›)
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®¡ç®—çš„æ˜¯é‡å»ºè¯¯å·®(MSE)ï¼Œè¶Šä½è¶Šå¥½
            loss = diffusion(
                x_start=motions, 
                context=text_embeddings, 
                src_mask=None 
            )
            total_loss += loss.item()

    avg_loss = total_loss / len(val_loader)
    diffusion.train() # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    return avg_loss

def train():
    # 1. åˆå§‹åŒ–
    set_seed(42)
    os.makedirs(Config.SAVE_DIR, exist_ok=True)
    os.makedirs(Config.LOG_DIR, exist_ok=True)
    
    device = torch.device(Config.DEVICE)
    print(f"ğŸš€ Training on device: {device}")
    
    writer = SummaryWriter(log_dir=Config.LOG_DIR)

    # ====================================================
    # 2. å‡†å¤‡æ•°æ®åŠ è½½å™¨ (Train & Val)
    # ====================================================
    print("ğŸ“‚ Loading Datasets...")
    
    # è®­ç»ƒé›†åŠ è½½å™¨
    train_loader = build_dataloader(
        lmdb_path=Config.LMDB_TRAIN_PATH,
        n_poses=Config.WINDOW_FRAMES,
        mean=Config.DATA_MEAN,
        std=Config.DATA_STD,
        batch_size=Config.BATCH_SIZE,
        shuffle=True,           # è®­ç»ƒé›†éœ€è¦æ‰“ä¹±
        num_workers=Config.NUM_WORKERS
    )
    
    # éªŒè¯é›†åŠ è½½å™¨
    val_loader = build_dataloader(
        lmdb_path=Config.LMDB_TEST_PATH,
        n_poses=Config.WINDOW_FRAMES,
        mean=Config.DATA_MEAN,
        std=Config.DATA_STD,
        batch_size=Config.BATCH_SIZE,
        shuffle=False,          # éªŒè¯é›†ä¸éœ€è¦æ‰“ä¹±
        num_workers=Config.NUM_WORKERS
    )
    print(f"âœ… Loaded: {len(train_loader)} train steps, {len(val_loader)} val steps per epoch.")

    # 3. åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ§  Initializing Models...")
    text_encoder = AutoModel.from_pretrained("bert-base-uncased").to(device)
    text_encoder.eval() # å†»ç»“ BERT
    for param in text_encoder.parameters():
        param.requires_grad = False

    model = Text2GestureModel(
        input_feats=Config.INPUT_FEATS,
        latent_dim=Config.LATENT_DIM,
        n_layers=Config.LAYERS,
        n_heads=Config.HEADS,
        dropout=Config.DROPOUT,
        text_dim=Config.TEXT_DIM
    ).to(device)

    diffusion = GaussianDiffusion(
        model=model,
        timesteps=Config.DIFFUSION_STEPS,
        loss_type=Config.LOSS_TYPE,
        beta_schedule=Config.BETA_SCHEDULE
    ).to(device)

    # 4. ä¼˜åŒ–å™¨ & æ¢å¤è®­ç»ƒé€»è¾‘
    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.0)
    
    start_epoch = 0
    global_step = 0
    best_val_loss = float('inf') # è®°å½•æœ€ä½³éªŒè¯æŸå¤±
    

    if Config.RESUME_CHECKPOINT and os.path.exists(Config.RESUME_CHECKPOINT):
        print(f"â™»ï¸ Resuming from: {Config.RESUME_CHECKPOINT}")
        ckpt = torch.load(Config.RESUME_CHECKPOINT, map_location=device)
        diffusion.load_state_dict(ckpt['model_state_dict'])
        optimizer.load_state_dict(ckpt['optimizer_state_dict'])
        start_epoch = ckpt['epoch'] + 1
        # å¦‚æœ checkpoint é‡Œå­˜äº† best_loss å°±è¯»å‡ºæ¥
        if 'best_val_loss' in ckpt:
            best_val_loss = ckpt['best_val_loss']
    else:
        print("âœ¨ Starting from scratch.")

    # ====================================================
    # 5. è®­ç»ƒå¾ªç¯
    # ====================================================
    print("ğŸ”¥ Start Training Loop...")
    
    for epoch in range(start_epoch, Config.EPOCHS):
        diffusion.train()
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{Config.EPOCHS}")
        loss_epoch = 0

        for batch in progress_bar:
            # --- Train Step ---
            motions = batch['x'].to(device)
            cond_ids = batch['cond'].to(device)
            cond_mask = batch['cond_mask'].to(device)

            with torch.no_grad():
                text_emb = text_encoder(input_ids=cond_ids, attention_mask=cond_mask).last_hidden_state

            # CFG Trick (10% unconditioned)
            if random.random() < 0.1:
                text_emb = torch.zeros_like(text_emb)

            loss = diffusion(motions, context=text_emb)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # --- Logging ---
            loss_val = loss.item()
            loss_epoch += loss_val
            global_step += 1
            progress_bar.set_postfix({"Loss": f"{loss_val:.4f}"})
            
            if global_step % 10 == 0:
                writer.add_scalar("Train/Loss", loss_val, global_step)

        avg_train_loss = loss_epoch / len(train_loader)
        writer.add_scalar("Train/Epoch_Loss", avg_train_loss, epoch)

        # ====================================================
        # 6. éªŒè¯å¾ªç¯ (Validation Loop)
        # ====================================================
        # æ¯éš” Config.EVAL_INTERVAL è½®ï¼Œæˆ–è€…æœ€åä¸€è½®ï¼Œè¿›è¡ŒéªŒè¯
        if (epoch + 1) % getattr(Config, 'EVAL_INTERVAL', 5) == 0:
            print(f"\nğŸ” Evaluating on Test Set...")
            val_loss = evaluate(diffusion, val_loader, text_encoder, device)
            print(f"    >> Val Loss: {val_loss:.5f} (Best: {best_val_loss:.5f})")
            
            writer.add_scalar("Val/Loss", val_loss, epoch)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_path = os.path.join(Config.SAVE_DIR, "best_model.pt")
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': diffusion.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_loss': best_val_loss,
                }, best_path)
                print(f"    ğŸ† New Record! Saved best model to {best_path}")

        # ====================================================
        # 7. å®šæœŸä¿å­˜ (Regular Checkpoint)
        # ====================================================
        if (epoch + 1) % Config.SAVE_INTERVAL == 0:
            save_path = os.path.join(Config.SAVE_DIR, f"model_epoch_{epoch+1}.pt")
            torch.save({
                'epoch': epoch,
                'model_state_dict': diffusion.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_train_loss,
            }, save_path)
            print(f"ğŸ’¾ Checkpoint saved: {save_path}")

    print("ğŸ‰ Training Finished!")
    writer.close()

if __name__ == '__main__':
    train()
import logging
import os
import pickle
import lmdb
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer
import pyarrow

# å¼•å…¥é¢„å¤„ç†å™¨
from src.data.data_preprocessor import DataPreprocessor

class TextMotionDataset(Dataset):
    def __init__(self, lmdb_dir, n_poses, subdivision_stride, pose_resampling_fps, data_mean, data_std, max_text_len=30):
        self.lmdb_dir = lmdb_dir

        print(f"ğŸ“‚ [Loader] æ­£åœ¨åŠ è½½ LMDB è·¯å¾„: {lmdb_dir}") # <--- åŠ è¿™è¡Œ
        self.n_poses = n_poses
        self.mean = np.array(data_mean).squeeze()
        self.std = np.array(data_std).squeeze()
        self.max_text_len = max_text_len
        
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        # ---------------------------------------------------------
        # âš ï¸ å…³é”®ä¿®æ”¹ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶ç”Ÿæˆ Cache
        # ---------------------------------------------------------
        preloaded_dir = lmdb_dir + '_cache_v2'
        
        if not os.path.exists(preloaded_dir):
            logging.info(f"Cache not found at {preloaded_dir}.")
            logging.info("ğŸš€ Starting automatic data preprocessing (slicing)...")
            logging.info("This may take a few minutes depending on dataset size.")
            
            processor = DataPreprocessor(
                clip_lmdb_dir=lmdb_dir,
                out_lmdb_dir=preloaded_dir,
                n_poses=n_poses,
                subdivision_stride=subdivision_stride, # é»˜è®¤åˆ‡ç‰‡æ­¥é•¿
                pose_resampling_fps=pose_resampling_fps
            )
            processor.run()
            logging.info("âœ… Preprocessing done.")
        else:
            logging.info(f"Using existing cache: {preloaded_dir}")
        # ---------------------------------------------------------

        self.lmdb_env = lmdb.open(preloaded_dir, readonly=True, lock=False)
        with self.lmdb_env.begin() as txn:
            self.n_samples = txn.stat()['entries']

    def __len__(self):
        return self.n_samples

    def __getitem__(self, idx):
        with self.lmdb_env.begin(write=False) as txn:
            key = '{:010}'.format(idx).encode('ascii')
            sample_bytes = txn.get(key)
            
            # ååºåˆ—åŒ–
            try:
                sample = pyarrow.deserialize(sample_bytes)
            except:
                sample = pickle.loads(sample_bytes)

            # ==========================================
            # ğŸ› ï¸ è§£åŒ…é€»è¾‘ (é€‚é… 4å…ƒç´  æˆ– 3å…ƒç´ )
            # ==========================================
            if len(sample) == 4:
                # æ ‡å‡†æ ¼å¼: [å•è¯, åŠ¨ä½œ, éŸ³é¢‘, è¾…åŠ©]
                word_list, pose_seq, audio, aux_info = sample
            
            elif len(sample) == 3:
                # å…¼å®¹æ—§æ ¼å¼: [éŸ³é¢‘, åŠ¨ä½œ, è¾…åŠ©]
                # è¿™ç§æƒ…å†µåº”è¯¥å¾ˆå°‘è§ï¼Œå› ä¸ºä½ çš„ DataPreprocessor åº”è¯¥ç”Ÿæˆ4ä¸ªå…ƒç´ çš„
                audio, pose_seq, aux_info = sample
                
                # å°è¯•è¡¥å…¨ word_list
                if isinstance(aux_info, dict) and 'words' in aux_info:
                    word_list = aux_info['words']
                elif isinstance(aux_info, dict) and 'text' in aux_info:
                    word_list = aux_info['text']
                else:
                    word_list = [['<unk>', 0.0, 0.0]] # æ‰¾ä¸åˆ°å°±ç»™ä¸ªç©ºçš„
            else:
                 # é‡åˆ°åæ•°æ®è¿”å› Noneï¼Œcollate_fn éœ€è¦é¢å¤–å¤„ç†ï¼Œæˆ–è€…ç›´æ¥æŠ¥é”™
                raise ValueError(f"Unknown data structure: len={len(sample)}")

        # ---------------------------------------------------------
        # 1. å¤„ç†æ–‡æœ¬ (Text Processing)
        # ---------------------------------------------------------
        # å°†å•è¯åˆ—è¡¨æ‹¼æ¥æˆå­—ç¬¦ä¸²
        # å‡è®¾ word_list é‡Œçš„ç»“æ„æ˜¯ [['hello', 0.1, 0.2], ...]
        if word_list and isinstance(word_list[0], (list, tuple)):
             text_str = " ".join([w[0] for w in word_list])
        else:
             text_str = " ".join(word_list) if word_list else ""

        # BERT Tokenizer
        tokenized = self.tokenizer(
            text_str,
            padding='max_length',
            truncation=True,
            max_length=self.max_text_len,
            return_tensors='pt',
            return_attention_mask=True
        )
        
        text_ids = tokenized['input_ids'].squeeze(0)
        text_mask = tokenized['attention_mask'].squeeze(0)

        # ---------------------------------------------------------
        # 2. å¤„ç†åŠ¨ä½œä¸å½’ä¸€åŒ– (Normalization)
        # ---------------------------------------------------------
        # Z-Score Normalization
        epsilon = 1e-6
        std_safe = np.clip(self.std, a_min=epsilon, a_max=None)
        pose_seq = (pose_seq - self.mean) / std_safe
        
        # è½¬ Tensor
        pose_seq = torch.from_numpy(pose_seq).float()
        audio = torch.from_numpy(audio).float()

        # è¿”å›å­—å…¸ (é€‚é… collate_fn)
        return {
            "motion": pose_seq,
            "text_ids": text_ids,
            "text_mask": text_mask,
            "audio": audio
        }

def collate_fn(batch):
    batch_motion = [item['motion'] for item in batch]
    batch_text_ids = [item['text_ids'] for item in batch]
    batch_text_mask = [item['text_mask'] for item in batch]
    
    motions = torch.stack(batch_motion)
    text_ids = torch.stack(batch_text_ids)
    text_masks = torch.stack(batch_text_mask)
    
    return {
        "x": motions,
        "cond": text_ids,
        "cond_mask": text_masks
    }

def build_dataloader(lmdb_path, n_poses, mean, std, batch_size, shuffle=True, num_workers=0):
    dataset = TextMotionDataset(
        lmdb_dir=lmdb_path,
        n_poses=n_poses,
        subdivision_stride=10, # æ­¥é•¿ï¼Œè¶Šå°ç”Ÿæˆçš„æ•°æ®è¶Šå¤š
        pose_resampling_fps=30,
        data_mean=mean,
        data_std=std
    )
    
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,
        drop_last=True
    )
    return loader
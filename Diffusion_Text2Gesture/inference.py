import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch
import numpy as np
import joblib as jl
import math
import pandas as pd
from scipy.signal import savgol_filter
from scipy.spatial.transform import Rotation as R
import sys
from scipy.ndimage import gaussian_filter1d



# --- æ·»åŠ è¿™å‡ è¡Œä»£ç  ---
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (/home/hti_2025/wei/mywork)
current_dir = os.path.dirname(os.path.abspath(__file__))
# æ„é€  src ç›®å½•çš„è·¯å¾„
src_dir = os.path.join(current_dir, 'src')
# å°† src åŠ å…¥åˆ° Python æœç´¢è·¯å¾„ä¸­
if src_dir not in sys.path:
    sys.path.append(src_dir)
# --------------------


# å¼•å…¥é…ç½®å’Œæ¨¡å‹
from src.config import Config
from src.model.model import Text2GestureModel
from src.model.diffusion import GaussianDiffusion 
from transformers import AutoTokenizer, AutoModel
from src.pymo.parsers import BVHParser
from src.pymo.writers import BVHWriter

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# âš ï¸ å¿…é¡»ä¸ä½ è®­ç»ƒæ—¶çš„å…³èŠ‚åˆ—è¡¨å®Œå…¨ä¸€è‡´
# ==========================================
TARGET_JOINTS = [
    'b_spine0', 'b_spine1', 'b_spine2', 'b_spine3', 
    'b_l_shoulder', 'b_l_arm', 'b_l_arm_twist', 'b_l_forearm', 'b_l_wrist_twist', 'b_l_wrist', 
    'b_r_shoulder', 'b_r_arm', 'b_r_arm_twist', 'b_r_forearm', 'b_r_wrist_twist', 'b_r_wrist', 
    'b_neck0', 'b_head'
]

# ==========================================
# 1. è¾…åŠ©å‡½æ•°
# ==========================================
def parse_tsv(tsv_path):
    words = []
    with open(tsv_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 3:
                words.append([parts[2], float(parts[0]), float(parts[1])])
    return words

def get_words_in_window(all_words, start_time, end_time):
    current_words = []
    for w in all_words:
        word_str, t_s, t_e = w
        word_center = (t_s + t_e) / 2
        if start_time <= word_center < end_time:
            current_words.append(word_str)
    if len(current_words) == 0:
        return "predicting motion"
    return " ".join(current_words)

def load_bert(device):
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    text_model = AutoModel.from_pretrained('bert-base-uncased').to(device)
    text_model.eval()
    return tokenizer, text_model

def get_text_embedding(text, tokenizer, text_model, device):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=30).to(device)
    with torch.no_grad():
        output = text_model(**inputs)
        text_emb = output.last_hidden_state 
    return text_emb

def cleanup_12d_data(poses):
    """SVD æ­£äº¤åŒ–ä¿®å¤å‡½æ•° (12D -> 6D)"""
    n_frames, n_joints, _ = poses.shape
    out_data = np.zeros((n_frames, n_joints, 6)) # 3 Pos + 3 Euler
    out_data[..., :3] = poses[..., :3]
    
    rot_mats = poses[..., 3:].reshape(n_frames, n_joints, 3, 3)
    for f in range(n_frames):
        for j in range(n_joints):
            mat = rot_mats[f, j]
            u, s, vt = np.linalg.svd(mat)
            clean_mat = np.dot(u, vt)
            if np.linalg.det(clean_mat) < 0:
                u[:, -1] *= -1
                clean_mat = np.dot(u, vt)
            r = R.from_matrix(clean_mat)
            # è¾“å‡º ZXY æ¬§æ‹‰è§’
            out_data[f, j, 3:] = r.as_euler('ZXY', degrees=True)
    return out_data

# ==========================================
# â­ æ–°å¢ï¼šæ‰‹åŠ¨å¯¼å‡º BVH å‡½æ•° (æ›¿ä»£ Pipeline)
# ==========================================
def export_bvh_manual(motion_data, ref_bvh_path, output_path):
    """
    motion_data: (Frames, 18, 6) 
                 -> [3 Pos, 3 Euler] 
                 -> å…¶ä¸­ Euler çš„é¡ºåºåœ¨ cleanup_12d_data é‡Œå¿…é¡»æ˜¯ r.as_euler('ZXY', degrees=True)
    ref_bvh_path: å‚è€ƒ BVH è·¯å¾„ï¼Œç”¨äºè¯»å–éª¨æ¶ç»“æ„å’Œ OFFSET
    output_path: æœ€ç»ˆç”Ÿæˆçš„ BVH ä¿å­˜è·¯å¾„
    """
    print(f"Exporting BVH manually using skeleton from: {ref_bvh_path}")
    
    # 1. è§£æå‚è€ƒ BVH è·å–éª¨æ¶
    parser = BVHParser()
    ref_data = parser.parse(ref_bvh_path)
    
    # 2. å‡†å¤‡æ–°çš„ DataFrame (é”å®š 30 FPS)
    n_frames = motion_data.shape[0]
    frame_time = 1.0 / 30.0 
    new_index = pd.to_timedelta(np.arange(n_frames) * frame_time, unit='s')
    
    # è·å–åˆ—åå¹¶åˆ›å»ºç©ºè¡¨
    cols = ref_data.values.columns
    new_values = pd.DataFrame(index=new_index, columns=cols)
    
    # 3. å¡«å……åˆå§‹å§¿æ€ (Rest Pose)
    # è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼šå®ƒä¿è¯äº†é‚£äº›æˆ‘ä»¬æ²¡ç”Ÿæˆçš„å…³èŠ‚ï¼ˆæ‰‹æŒ‡ã€ä¸‹åŠèº«ï¼‰ä¿æŒé™æ­¢ï¼Œè€Œä¸æ˜¯ä¹±é£
    ref_frame_0 = ref_data.values.iloc[0].values
    new_values.iloc[:, :] = np.tile(ref_frame_0, (n_frames, 1))
    
    # ========================================================
    # ğŸš€ æ ¸å¿ƒä¿®æ­£ï¼šé€šé“é‡æ˜ å°„ (Channel Remapping)
    # ========================================================
    # æ ¹æ®è°ƒè¯•ç»“è®ºï¼š
    # - æ¨¡å‹è¾“å‡º ch_1 æ˜¯ä¸»åŠ›åŠ¨ä½œ (å¤§å¹…åº¦æ•°å€¼)
    # - BVH çš„ Xè½´æ˜¯ Twist (æ‹§æ¯›å·¾/è‡ªè½¬)
    # - BVH çš„ Yè½´æ˜¯ Lift (æŠ¬èƒ³è†Š)
    # -> å¿…é¡»æŠŠ ch_1 èµ‹ç»™ Yè½´ï¼
    
    for i, joint_name in enumerate(TARGET_JOINTS):
        # æå–æ¨¡å‹ç”Ÿæˆçš„ä¸‰ä¸ªæ—‹è½¬é€šé“
        # æ³¨æ„ï¼šmotion_data çš„å‰3ä½æ˜¯ä½ç½®(é€šå¸¸ä¸ç”¨)ï¼Œå3ä½æ˜¯æ—‹è½¬
        ch_0 = motion_data[:, i, 3] # é€šå¸¸å¯¹åº” Z (Swing/æ‘†åŠ¨)
        ch_1 = motion_data[:, i, 4] # é€šå¸¸å¯¹åº” X (ä½†è¿™é‡Œæˆ‘ä»¬è¦æ¢ç»™ Y)
        ch_2 = motion_data[:, i, 5] # é€šå¸¸å¯¹åº” Y (ä½†è¿™é‡Œæˆ‘ä»¬è¦æ¢ç»™ X)
        
        # 1. è®¾ç½® Z è½´ (å‰åæ‘†åŠ¨) - ä¿æŒç›´é€š
        if f'{joint_name}_Zrotation' in cols:
            new_values[f'{joint_name}_Zrotation'] = ch_0
            
        # 2. è®¾ç½® Y è½´ (æŠ¬èƒ³è†Š) - âœ… ä¿®æ­£ç‚¹ï¼šæ¥æ”¶ä¸»åŠ›æ•°æ® ch_1
        if f'{joint_name}_Yrotation' in cols:
            new_values[f'{joint_name}_Yrotation'] = ch_1  
            
        # 3. è®¾ç½® X è½´ (æ‹§æ¯›å·¾/å¾®è°ƒ) - âœ… ä¿®æ­£ç‚¹ï¼šæ¥æ”¶æ¬¡è¦æ•°æ® ch_2
        if f'{joint_name}_Xrotation' in cols:
            new_values[f'{joint_name}_Xrotation'] = ch_2

    # ========================================================

    # 4. æ›´æ–°æ•°æ®
    ref_data.values = new_values

    # 5. ç®€å•æ£€æŸ¥ï¼ˆç¡®ä¿æ²¡æœ‰å†™å…¥å…¨0æ•°æ®ï¼‰
    check_col = 'b_r_arm_Yrotation' # æ£€æŸ¥ Y è½´æ˜¯å¦æœ‰æ•°æ®
    if check_col in new_values.columns:
        col_data = new_values[check_col].values
        range_val = np.max(col_data) - np.min(col_data)
        print(f"ğŸ“Š æ•°æ®å†™å…¥æ£€æŸ¥ ({check_col}): å˜åŒ–å¹…åº¦ = {range_val:.4f}")
        if range_val < 0.1:
            print("âš ï¸ è­¦å‘Šï¼šåŠ¨ä½œå¹…åº¦æå°ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥åå½’ä¸€åŒ–æˆ–æ¨¡å‹æ•ˆæœã€‚")

    # 6. è‡ªåŠ¨åˆ›å»ºç›®å½•å¹¶ä¿å­˜
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        
    writer = BVHWriter()
    with open(output_path, 'w') as f:
        writer.write(ref_data, f)
        
    print(f"âœ… BVH saved to: {os.path.abspath(output_path)}")
    print(f"Exporting BVH manually using skeleton from: {ref_bvh_path}")
    parser = BVHParser()
    ref_data = parser.parse(ref_bvh_path)
    
    # å‡†å¤‡ DataFrame
    n_frames = motion_data.shape[0]
    frame_time = 1.0 / 30.0 
    new_index = pd.to_timedelta(np.arange(n_frames) * frame_time, unit='s')
    cols = ref_data.values.columns
    new_values = pd.DataFrame(index=new_index, columns=cols)
    ref_frame_0 = ref_data.values.iloc[0].values
    new_values.iloc[:, :] = np.tile(ref_frame_0, (n_frames, 1))
    
    # ========================================================
    # ğŸš€ æœ€ç»ˆæ˜ å°„ä¿®æ­£ (åŸºäºä½ çš„è§‚å¯Ÿ: X=Twist, Y=Lift, Z=Swing)
    # ========================================================
    
    for i, joint_name in enumerate(TARGET_JOINTS):
        # æå–æ¨¡å‹ç”Ÿæˆçš„ä¸‰ä¸ªé€šé“
        # æ ¹æ®ç»éªŒï¼š
        # ch_1 é€šå¸¸æ˜¯å¹…åº¦æœ€å¤§çš„ (ä¸Šä¸‹æŠ¬)
        # ch_0 é€šå¸¸æ˜¯ç¬¬äºŒå¤§çš„ (å‰åæ‘†)
        # ch_2 é€šå¸¸æ˜¯æœ€å°çš„ (è‡ªè½¬)
        ch_0 = motion_data[:, i, 3] 
        ch_1 = motion_data[:, i, 4] 
        ch_2 = motion_data[:, i, 5] 
        
        # 1. Z è½´ (æ‘†èƒ³è†Š) <- æ¥æ”¶ ch_0
        if f'{joint_name}_Zrotation' in cols:
            new_values[f'{joint_name}_Zrotation'] = ch_0
            
        # 2. Y è½´ (æŠ¬èƒ³è†Š - æ ¸å¿ƒåŠ¨ä½œ!) <- æ¥æ”¶ ch_1 (ä¹‹å‰é”™ç»™Xçš„æ•°æ®)
        if f'{joint_name}_Yrotation' in cols:
            new_values[f'{joint_name}_Yrotation'] = ch_1  # âœ… ä¿®æ­£ï¼šä¸»åŠ›æ•°æ®ç»™ Y
            
        # 3. X è½´ (æ‹§æ¯›å·¾ - è¾…åŠ©åŠ¨ä½œ) <- æ¥æ”¶ ch_2
        if f'{joint_name}_Xrotation' in cols:
            new_values[f'{joint_name}_Xrotation'] = ch_2  # âœ… ä¿®æ­£ï¼šæ¬¡è¦æ•°æ®ç»™ X

    # ========================================================

    ref_data.values = new_values
    
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        
    writer = BVHWriter()
    with open(output_path, 'w') as f:
        writer.write(ref_data, f)
    print(f"âœ… BVH saved to: {output_path}")
# ==========================================
# 3. é•¿åºåˆ—ç”Ÿæˆé€»è¾‘
# ==========================================
# def generate_long_sequence(cfg, model, diffusion, tokenizer, text_model, tsv_path):
#     words = parse_tsv(tsv_path)
#     if not words: return None
    
#     total_duration = words[-1][2]
#     FPS = 30 
#     CLIP_FRAMES = 120 
#     CLIP_DURATION = CLIP_FRAMES / FPS 
#     num_clips = math.ceil(total_duration / CLIP_DURATION)
    
#     print(f"Total Duration: {total_duration:.2f}s, Generating {num_clips} clips...")
#     generated_clips = []
    
#     for i in range(num_clips):
#         start_t = i * CLIP_DURATION
#         end_t = start_t + CLIP_DURATION
#         window_text = get_words_in_window(words, start_t, end_t)
#         print(f"Clip {i+1}/{num_clips} [{start_t:.1f}s-{end_t:.1f}s]: '{window_text}'")
        
#         text_emb = get_text_embedding(window_text, tokenizer, text_model, device)
#         sample_shape = (1, CLIP_FRAMES, cfg.INPUT_FEATS)
        
#         # ç”Ÿæˆ
#         raw_motion = diffusion.sample(sample_shape, text_emb, src_mask=None, guidance_scale=2.5)
#         raw_motion = raw_motion.squeeze(0).cpu().numpy()
#         generated_clips.append(raw_motion)
        
#     full_motion = np.vstack(generated_clips)
#     target_frames = int(total_duration * FPS)
#     full_motion = full_motion[:target_frames]
    
#     return full_motion

def generate_long_sequence(cfg, model, diffusion, tokenizer, text_model, tsv_path):
    # 1. è§£æ TSV
    words = parse_tsv(tsv_path)
    if not words: return None
    
    # ğŸ”´ ä¿®æ­£ç‚¹ï¼šä¸è¦åªå–æœ€åä¸€ä¸ªï¼Œè¦æ‰¾æœ€å¤§å€¼ï¼
    # éå†æ‰€æœ‰è¯ï¼Œæ‰¾åˆ°æœ€å¤§çš„ end_time
    total_duration = max([w[2] for w in words])
    
    # æˆ–è€…æ‰‹åŠ¨å¼ºåˆ¶æŒ‡å®šæ—¶é•¿ï¼ˆå¦‚æœä½ ç¡®ä¿¡æ˜¯ 60 ç§’ï¼‰
    # total_duration = 60.0 
    
    FPS = 30 
    CLIP_FRAMES = 120 
    CLIP_DURATION = CLIP_FRAMES / FPS 
    num_clips = math.ceil(total_duration / CLIP_DURATION)
    
    print(f"ğŸ¯ çœŸå®æ—¶é•¿: {total_duration:.2f}s | è®¡åˆ’ç”Ÿæˆ: {num_clips} æ®µ")
    
    generated_clips = []
    
    for i in range(num_clips):
        start_t = i * CLIP_DURATION
        end_t = start_t + CLIP_DURATION
        window_text = get_words_in_window(words, start_t, end_t)
        print(f"   - Clip {i+1}/{num_clips} [{start_t:.1f}s-{end_t:.1f}s]: '{window_text}'")
        
        text_emb = get_text_embedding(window_text, tokenizer, text_model, device)
        # æ³¨æ„ï¼šè¿™é‡Œå¤§å°å†™è¦å’Œ Config ç±»ä¸€è‡´ï¼Œä¹‹å‰æ˜¯ input_feats
        sample_shape = (1, CLIP_FRAMES, cfg.INPUT_FEATS) 
        
        # ç”Ÿæˆ
        raw_motion = diffusion.sample(sample_shape, text_emb, src_mask=None, guidance_scale=2.5)
        raw_motion = raw_motion.squeeze(0).cpu().numpy()
        generated_clips.append(raw_motion)
        
    full_motion = np.vstack(generated_clips)
    
    # è®¡ç®—ç›®æ ‡å¸§æ•°
    target_frames = int(total_duration * FPS)
    
    # æˆªå–
    if full_motion.shape[0] > target_frames:
        print(f"âœ‚ï¸  è£å‰ªå¤šä½™å¸§æ•°: {full_motion.shape[0]} -> {target_frames}")
        full_motion = full_motion[:target_frames]
    
    return full_motion

# ==========================================
# 4. ä¸»å…¥å£
# ==========================================
def main():
    # ---------------------------------------------------------
    # ğŸ‘‡ é…ç½®è·¯å¾„ ğŸ‘‡
    # ---------------------------------------------------------
    MY_CKPT_PATH = "/home/hti_2025/wei/mywork/checkpoints/model_epoch_600.pt" 
    MY_TSV_PATH = "/home/hti_2025/hti_2025/dataset/genea2023_dataset/tst/main-agent/tsv/tst_2023_v0_001_main-agent.tsv"
    
    # â­ æ–°å¢ï¼šå‚è€ƒ BVH è·¯å¾„ (ç”¨äºå€Ÿç”¨éª¨æ¶)
    # è¯·å¡«å…¥åŒåçš„ bvh æ–‡ä»¶è·¯å¾„ï¼Œæˆ–è€…ä»»ä½•ä¸€ä¸ªæœ‰æ•ˆçš„è®­ç»ƒé›† bvh æ–‡ä»¶
    MY_REF_BVH_PATH = "/home/hti_2025/hti_2025/dataset/genea2023_dataset/trn/interloctr/bvh/trn_2023_v0_000_interloctr.bvh"
    
    MY_OUTPUT_PATH = "/home/hti_2025/wei/mywork/results/long_gaussian90_2motion600.bvh"
    # ---------------------------------------------------------

    if not os.path.exists(MY_REF_BVH_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°å‚è€ƒ BVH æ–‡ä»¶: {MY_REF_BVH_PATH}")
        print("è¯·å°† MY_REF_BVH_PATH æŒ‡å‘ä»»ä½•ä¸€ä¸ªåŸå§‹çš„ .bvh æ–‡ä»¶ï¼Œä»¥ä¾¿è„šæœ¬è¯»å–éª¨éª¼ç»“æ„ã€‚")
        return

    cfg = Config()
    tokenizer, text_model = load_bert(device)
    
    print("Initializing model...")
    model = Text2GestureModel(
        input_feats=cfg.INPUT_FEATS, latent_dim=cfg.LATENT_DIM,
        n_heads=cfg.HEADS, n_layers=cfg.LAYERS, text_dim=768
    ).to(device)
    
    diffusion = GaussianDiffusion(
        model, timesteps=cfg.DIFFUSION_STEPS, loss_type='l2', beta_schedule='cosine'
    ).to(device)
    
    # åŠ è½½æƒé‡ (ç›´æ¥åŠ è½½ç»™ diffusion)
    print(f"Loading checkpoint {MY_CKPT_PATH}...")
    checkpoint = torch.load(MY_CKPT_PATH, map_location=device)
    diffusion.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)
    
    # è·å– mean/std
    data_mean = checkpoint.get('data_mean', np.zeros(cfg.INPUT_FEATS))
    data_std = checkpoint.get('data_std', np.ones(cfg.INPUT_FEATS))
    
    # æ‰§è¡Œç”Ÿæˆ
    full_raw_motion = generate_long_sequence(cfg, model, diffusion, tokenizer, text_model, MY_TSV_PATH)
    
    if full_raw_motion is None: return

    print("Post-processing full sequence...")
    
    # åå½’ä¸€åŒ–
    data_std = np.clip(data_std, a_min=1e-6, a_max=None)
    denorm_motion = full_raw_motion * data_std + data_mean

    # æ–¹å¼ A: ä½¿ç”¨é«˜æ–¯å¹³æ»‘ (æ¨èç”¨äºå»æŠ–åŠ¨)
    # sigma è¶Šå¤§è¶Šå¹³æ»‘ï¼Œä½†ä¹Ÿè¶Š"è‚‰" (è¿Ÿç¼“)ã€‚
    # å»ºè®®èŒƒå›´: 1.0 (è½»å¾®) ~ 4.0 (å¼ºåŠ›)
    denorm_motion = gaussian_filter1d(denorm_motion, sigma=9.0, axis=0)
    
    # å¹³æ»‘
    # denorm_motion = savgol_filter(denorm_motion, window_length=15, polyorder=2, axis=0)
    
    # SVD ä¿®å¤ (Frames, 18, 6)
    denorm_motion = denorm_motion.reshape(denorm_motion.shape[0], -1, 12)
    clean_motion = cleanup_12d_data(denorm_motion)
    
    # â­ æ‰‹åŠ¨å¯¼å‡º (ä¸å†ä½¿ç”¨ pipeline.inverse_transform)
    export_bvh_manual(clean_motion, MY_REF_BVH_PATH, MY_OUTPUT_PATH)

if __name__ == '__main__':
    main()
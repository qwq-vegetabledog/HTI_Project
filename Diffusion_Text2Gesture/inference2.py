import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch
import numpy as np
import joblib as jl
import math
import pandas as pd
from scipy.signal import savgol_filter
from scipy.spatial.transform import Rotation as R
import sys
from scipy.ndimage import gaussian_filter1d
import os
import glob
from tqdm import tqdm  # å¦‚æœæ²¡æœ‰å®‰è£…ï¼Œè¯· pip install tqdm


# --- æ·»åŠ è¿™å‡ è¡Œä»£ç  ---
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (/home/hti_2025/wei/mywork)
current_dir = os.path.dirname(os.path.abspath(__file__))
# æ„é€  src ç›®å½•çš„è·¯å¾„
src_dir = os.path.join(current_dir, 'src')
# å°† src åŠ å…¥åˆ° Python æœç´¢è·¯å¾„ä¸­
if src_dir not in sys.path:
    sys.path.append(src_dir)
# --------------------


# å¼•å…¥é…ç½®å’Œæ¨¡å‹
from src.config import Config
from src.model.model import Text2GestureModel
from src.model.diffusion import GaussianDiffusion 
from transformers import AutoTokenizer, AutoModel
from src.pymo.parsers import BVHParser
from src.pymo.writers import BVHWriter

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# âš ï¸ å¿…é¡»ä¸ä½ è®­ç»ƒæ—¶çš„å…³èŠ‚åˆ—è¡¨å®Œå…¨ä¸€è‡´
# ==========================================
TARGET_JOINTS = [
    'b_spine0', 'b_spine1', 'b_spine2', 'b_spine3', 
    'b_l_shoulder', 'b_l_arm', 'b_l_arm_twist', 'b_l_forearm', 'b_l_wrist_twist', 'b_l_wrist', 
    'b_r_shoulder', 'b_r_arm', 'b_r_arm_twist', 'b_r_forearm', 'b_r_wrist_twist', 'b_r_wrist', 
    'b_neck0', 'b_head'
]

# ==========================================
# 1. è¾…åŠ©å‡½æ•°
# ==========================================
def parse_tsv(tsv_path):
    words = []
    with open(tsv_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 3:
                words.append([parts[2], float(parts[0]), float(parts[1])])
    return words

def get_words_in_window(all_words, start_time, end_time):
    current_words = []
    for w in all_words:
        word_str, t_s, t_e = w
        word_center = (t_s + t_e) / 2
        if start_time <= word_center < end_time:
            current_words.append(word_str)
    if len(current_words) == 0:
        return "predicting motion"
    return " ".join(current_words)

def load_bert(device):
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    text_model = AutoModel.from_pretrained('bert-base-uncased').to(device)
    text_model.eval()
    return tokenizer, text_model

def get_text_embedding(text, tokenizer, text_model, device):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=30).to(device)
    with torch.no_grad():
        output = text_model(**inputs)
        text_emb = output.last_hidden_state 
    return text_emb

def cleanup_12d_data(poses):
    """
    SVD Orthogonalization (12D -> 6D) with Auto-Reshape support.
    """
    # ==========================================
    # [FIX] Auto-Reshape: Handle 2D input (Frames, 216)
    # ==========================================
    if len(poses.shape) == 2:
        n_frames = poses.shape[0]
        # Calculate joints: Total Dim / 12 (3 Pos + 9 RotMat elements)
        n_joints = poses.shape[1] // 12
        poses = poses.reshape(n_frames, n_joints, 12)
    # ==========================================

    # Original Logic
    n_frames, n_joints, _ = poses.shape
    out_data = np.zeros((n_frames, n_joints, 6)) # 3 Pos + 3 Euler
    
    # Copy Position
    out_data[..., :3] = poses[..., :3]
    
    # Reshape rotation part to 3x3 matrices
    rot_mats = poses[..., 3:].reshape(n_frames, n_joints, 3, 3)
    
    for f in range(n_frames):
        for j in range(n_joints):
            mat = rot_mats[f, j]
            
            # SVD to enforce orthogonality
            u, s, vt = np.linalg.svd(mat)
            clean_mat = np.dot(u, vt)
            
            # Correct reflection
            if np.linalg.det(clean_mat) < 0:
                u[:, -1] *= -1
                clean_mat = np.dot(u, vt)
            
            r = R.from_matrix(clean_mat)
            
            # Output ZXY Euler angles (degrees=True, NO unwrap)
            out_data[f, j, 3:] = r.as_euler('ZXY', degrees=True)
            
    return out_data
# ==========================================
# â­ æ–°å¢ï¼šæ‰‹åŠ¨å¯¼å‡º BVH å‡½æ•° (æ›¿ä»£ Pipeline)
# ==========================================
def export_bvh_manual(motion_data, ref_bvh_path, output_path):
    """
    motion_data: (Frames, 18, 6) 
                 -> [3 Pos, 3 Euler] 
                 -> å…¶ä¸­ Euler çš„é¡ºåºåœ¨ cleanup_12d_data é‡Œå¿…é¡»æ˜¯ r.as_euler('ZXY', degrees=True)
    ref_bvh_path: å‚è€ƒ BVH è·¯å¾„ï¼Œç”¨äºè¯»å–éª¨æ¶ç»“æ„å’Œ OFFSET
    output_path: æœ€ç»ˆç”Ÿæˆçš„ BVH ä¿å­˜è·¯å¾„
    """
    print(f"Exporting BVH manually using skeleton from: {ref_bvh_path}")
    
    # 1. è§£æå‚è€ƒ BVH è·å–éª¨æ¶
    parser = BVHParser()
    ref_data = parser.parse(ref_bvh_path)
    
    # 2. å‡†å¤‡æ–°çš„ DataFrame (é”å®š 30 FPS)
    n_frames = motion_data.shape[0]
    frame_time = 1.0 / 30.0 
    new_index = pd.to_timedelta(np.arange(n_frames) * frame_time, unit='s')
    
    # è·å–åˆ—åå¹¶åˆ›å»ºç©ºè¡¨
    cols = ref_data.values.columns
    new_values = pd.DataFrame(index=new_index, columns=cols)
    
    # 3. å¡«å……åˆå§‹å§¿æ€ (Rest Pose)
    # è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼šå®ƒä¿è¯äº†é‚£äº›æˆ‘ä»¬æ²¡ç”Ÿæˆçš„å…³èŠ‚ï¼ˆæ‰‹æŒ‡ã€ä¸‹åŠèº«ï¼‰ä¿æŒé™æ­¢ï¼Œè€Œä¸æ˜¯ä¹±é£
    ref_frame_0 = ref_data.values.iloc[0].values
    new_values.iloc[:, :] = np.tile(ref_frame_0, (n_frames, 1))
    
    # ========================================================
    # ğŸš€ æ ¸å¿ƒä¿®æ­£ï¼šé€šé“é‡æ˜ å°„ (Channel Remapping)
    # ========================================================
    # æ ¹æ®è°ƒè¯•ç»“è®ºï¼š
    # - æ¨¡å‹è¾“å‡º ch_1 æ˜¯ä¸»åŠ›åŠ¨ä½œ (å¤§å¹…åº¦æ•°å€¼)
    # - BVH çš„ Xè½´æ˜¯ Twist (æ‹§æ¯›å·¾/è‡ªè½¬)
    # - BVH çš„ Yè½´æ˜¯ Lift (æŠ¬èƒ³è†Š)
    # -> å¿…é¡»æŠŠ ch_1 èµ‹ç»™ Yè½´ï¼
    
    for i, joint_name in enumerate(TARGET_JOINTS):
        # æå–æ¨¡å‹ç”Ÿæˆçš„ä¸‰ä¸ªæ—‹è½¬é€šé“
        # æ³¨æ„ï¼šmotion_data çš„å‰3ä½æ˜¯ä½ç½®(é€šå¸¸ä¸ç”¨)ï¼Œå3ä½æ˜¯æ—‹è½¬
        ch_0 = motion_data[:, i, 3] # é€šå¸¸å¯¹åº” Z (Swing/æ‘†åŠ¨)
        ch_1 = motion_data[:, i, 4] # é€šå¸¸å¯¹åº” X (ä½†è¿™é‡Œæˆ‘ä»¬è¦æ¢ç»™ Y)
        ch_2 = motion_data[:, i, 5] # é€šå¸¸å¯¹åº” Y (ä½†è¿™é‡Œæˆ‘ä»¬è¦æ¢ç»™ X)
        
        # 1. è®¾ç½® Z è½´ (å‰åæ‘†åŠ¨) - ä¿æŒç›´é€š
        if f'{joint_name}_Zrotation' in cols:
            new_values[f'{joint_name}_Zrotation'] = ch_0
            
        # 2. è®¾ç½® Y è½´ (æŠ¬èƒ³è†Š) - âœ… ä¿®æ­£ç‚¹ï¼šæ¥æ”¶ä¸»åŠ›æ•°æ® ch_1
        if f'{joint_name}_Yrotation' in cols:
            new_values[f'{joint_name}_Yrotation'] = ch_1  
            
        # 3. è®¾ç½® X è½´ (æ‹§æ¯›å·¾/å¾®è°ƒ) - âœ… ä¿®æ­£ç‚¹ï¼šæ¥æ”¶æ¬¡è¦æ•°æ® ch_2
        if f'{joint_name}_Xrotation' in cols:
            new_values[f'{joint_name}_Xrotation'] = ch_2

    # ========================================================

    # 4. æ›´æ–°æ•°æ®
    ref_data.values = new_values

    # 5. ç®€å•æ£€æŸ¥ï¼ˆç¡®ä¿æ²¡æœ‰å†™å…¥å…¨0æ•°æ®ï¼‰
    check_col = 'b_r_arm_Yrotation' # æ£€æŸ¥ Y è½´æ˜¯å¦æœ‰æ•°æ®
    if check_col in new_values.columns:
        col_data = new_values[check_col].values
        range_val = np.max(col_data) - np.min(col_data)
        print(f"ğŸ“Š æ•°æ®å†™å…¥æ£€æŸ¥ ({check_col}): å˜åŒ–å¹…åº¦ = {range_val:.4f}")
        if range_val < 0.1:
            print("âš ï¸ è­¦å‘Šï¼šåŠ¨ä½œå¹…åº¦æå°ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥åå½’ä¸€åŒ–æˆ–æ¨¡å‹æ•ˆæœã€‚")

    # 6. è‡ªåŠ¨åˆ›å»ºç›®å½•å¹¶ä¿å­˜
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        
    writer = BVHWriter()
    with open(output_path, 'w') as f:
        writer.write(ref_data, f)
        
    print(f"âœ… BVH saved to: {os.path.abspath(output_path)}")
    print(f"Exporting BVH manually using skeleton from: {ref_bvh_path}")
    parser = BVHParser()
    ref_data = parser.parse(ref_bvh_path)
    
    # å‡†å¤‡ DataFrame
    n_frames = motion_data.shape[0]
    frame_time = 1.0 / 30.0 
    new_index = pd.to_timedelta(np.arange(n_frames) * frame_time, unit='s')
    cols = ref_data.values.columns
    new_values = pd.DataFrame(index=new_index, columns=cols)
    ref_frame_0 = ref_data.values.iloc[0].values
    new_values.iloc[:, :] = np.tile(ref_frame_0, (n_frames, 1))
    
    # ========================================================
    # ğŸš€ æœ€ç»ˆæ˜ å°„ä¿®æ­£ (åŸºäºä½ çš„è§‚å¯Ÿ: X=Twist, Y=Lift, Z=Swing)
    # ========================================================

    # ========================================================
    # ğŸš€ æœ€ç»ˆæ˜ å°„ä¿®æ­£ (åŸºäºä½ çš„è§‚å¯Ÿ: X=Twist, Y=Lift, Z=Swing)
    # ========================================================
    
    # ========================================================
    # ğŸ› ï¸ å…³èŠ‚é™åˆ¶é…ç½® (Joint Constraints)
    # ========================================================
    # å®šä¹‰ä½ è¦é™åˆ¶çš„å…³èŠ‚åŠå…¶è¡°å‡ç³»æ•° (0.0=ä¸åŠ¨, 1.0=åŸæ ·, 0.5=å¹…åº¦å‡åŠ)
    # 0.0 = å®Œå…¨å†»ç»“ (ä¸åŠ¨)
    # 1.0 = å®Œå…¨åŸæ · (ç–¯åŠ¨)
    
    DAMPING_CONFIG = {
        # === æ ¸å¿ƒèº¯å¹² (ç¨³ä½ä¸‹ç›˜) ===
        'b_spine0': 0.01,  # è…°åˆ«ä¹±æ‰­
        'b_spine1': 0.05,
        'b_spine2': 0.06,
        'b_spine3': 0.07,
        
        # === å¤´é¢ˆ (æ‹’ç»æ‘‡å¤´æ™ƒè„‘) ===
        'b_neck0': 0.03,
        'b_head': 0.03,    # å¤´è¦ç¨³
        
        # === è‚©è†€ (æ‹’ç»ç´§å¼ è€¸è‚©) ===
        'b_l_shoulder': 0.05,
        'b_r_shoulder': 0.05,
        
        # === å¤§è‡‚ (æ§åˆ¶æŒ¥æ‰‹å¹…åº¦ï¼Œæœ€å…³é”®!) ===
        'b_l_arm': 0.3,   # ç¨å¾®æ”¶æ•›ä¸€ç‚¹
        'b_r_arm': 0.3,
        'twist': 0.02,     # âš ï¸ å‡¡æ˜¯åå­—å¸¦ twist çš„ï¼Œç»Ÿç»ŸæŒ‰æ­»ï¼Œé˜²æ­¢æ‰­æ›²
        
        # === å°è‡‚ä¸æ‰‹ (ä¿ç•™ç»†èŠ‚) ===
        'b_l_forearm': 0.5, # è®©èƒ³è†Šè‚˜çµæ´»ç‚¹
        'b_r_forearm': 0.5,
        'b_l_wrist': 0.5,   # æ‰‹æŒåŠ¨ä½œå¯ä»¥ä¸°å¯Œç‚¹
        'b_r_wrist': 0.5,
    }

    print("åº”ç”¨å…³èŠ‚é™åˆ¶...")

    
    for i, joint_name in enumerate(TARGET_JOINTS):
        # æå–æ¨¡å‹ç”Ÿæˆçš„ä¸‰ä¸ªé€šé“
        # æ ¹æ®ç»éªŒï¼š
        # ch_1 é€šå¸¸æ˜¯å¹…åº¦æœ€å¤§çš„ (ä¸Šä¸‹æŠ¬)
        # ch_0 é€šå¸¸æ˜¯ç¬¬äºŒå¤§çš„ (å‰åæ‘†)
        # ch_2 é€šå¸¸æ˜¯æœ€å°çš„ (è‡ªè½¬)
        ch_0 = motion_data[:, i, 3] 
        ch_1 = motion_data[:, i, 4] 
        ch_2 = motion_data[:, i, 5]
        # ğŸ” æ£€æŸ¥æ˜¯å¦éœ€è¦é™åˆ¶è¿™ä¸ªå…³èŠ‚
        # æˆ‘ä»¬æ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼Œæ¯”å¦‚é…ç½®å†™ 'arm'ï¼Œåˆ™æ‰€æœ‰åå­—å¸¦ arm çš„éƒ½ä¼šè¢«é™åˆ¶
        damping_factor = 1.0
        for key, factor in DAMPING_CONFIG.items():
            if key in joint_name:
                damping_factor = factor
                break
        
        # å¦‚æœç³»æ•°å°äº 1.0ï¼Œæ‰§è¡Œã€å‘å‡å€¼æ”¶ç¼©ã€‘ç®—æ³•
        if damping_factor < 1.0:
            # ç®—æ³•å…¬å¼: æ–°å€¼ = å‡å€¼ + (åŸå€¼ - å‡å€¼) * ç³»æ•°
            # è¿™æ ·ä¿è¯äº†åŠ¨ä½œå¹…åº¦å˜å°ï¼Œä½†äººä¸ä¼šæ­ªæ‰
            ch_0 = np.mean(ch_0) + (ch_0 - np.mean(ch_0)) * damping_factor
            ch_1 = np.mean(ch_1) + (ch_1 - np.mean(ch_1)) * damping_factor
            ch_2 = np.mean(ch_2) + (ch_2 - np.mean(ch_2)) * damping_factor

        # --- ä¸‹é¢æ˜¯ä¹‹å‰çš„æ˜ å°„é€»è¾‘ (ä¿æŒä¸å˜) ---

        
        # 1. Z è½´ (æ‘†èƒ³è†Š) <- æ¥æ”¶ ch_0
        if f'{joint_name}_Zrotation' in cols:
            new_values[f'{joint_name}_Zrotation'] = ch_0
            
        # 2. Y è½´ (æŠ¬èƒ³è†Š - æ ¸å¿ƒåŠ¨ä½œ!) <- æ¥æ”¶ ch_1 (ä¹‹å‰é”™ç»™Xçš„æ•°æ®)
        if f'{joint_name}_Yrotation' in cols:
            new_values[f'{joint_name}_Yrotation'] = ch_1  # âœ… ä¿®æ­£ï¼šä¸»åŠ›æ•°æ®ç»™ Y
            
        # 3. X è½´ (æ‹§æ¯›å·¾ - è¾…åŠ©åŠ¨ä½œ) <- æ¥æ”¶ ch_2
        if f'{joint_name}_Xrotation' in cols:
            new_values[f'{joint_name}_Xrotation'] = ch_2  # âœ… ä¿®æ­£ï¼šæ¬¡è¦æ•°æ®ç»™ X

    # ========================================================

    ref_data.values = new_values
    
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        
    writer = BVHWriter()
    with open(output_path, 'w') as f:
        writer.write(ref_data, f)
    print(f"âœ… BVH saved to: {output_path}")
# ==========================================
# 3. é•¿åºåˆ—ç”Ÿæˆé€»è¾‘
# ==========================================
# def generate_long_sequence(cfg, model, diffusion, tokenizer, text_model, tsv_path):
#     words = parse_tsv(tsv_path)
#     if not words: return None
    
#     total_duration = words[-1][2]
#     FPS = 30 
#     CLIP_FRAMES = 120 
#     CLIP_DURATION = CLIP_FRAMES / FPS 
#     num_clips = math.ceil(total_duration / CLIP_DURATION)
    
#     print(f"Total Duration: {total_duration:.2f}s, Generating {num_clips} clips...")
#     generated_clips = []
    
#     for i in range(num_clips):
#         start_t = i * CLIP_DURATION
#         end_t = start_t + CLIP_DURATION
#         window_text = get_words_in_window(words, start_t, end_t)
#         print(f"Clip {i+1}/{num_clips} [{start_t:.1f}s-{end_t:.1f}s]: '{window_text}'")
        
#         text_emb = get_text_embedding(window_text, tokenizer, text_model, device)
#         sample_shape = (1, CLIP_FRAMES, cfg.INPUT_FEATS)
        
#         # ç”Ÿæˆ
#         raw_motion = diffusion.sample(sample_shape, text_emb, src_mask=None, guidance_scale=2.5)
#         raw_motion = raw_motion.squeeze(0).cpu().numpy()
#         generated_clips.append(raw_motion)
        
#     full_motion = np.vstack(generated_clips)
#     target_frames = int(total_duration * FPS)
#     full_motion = full_motion[:target_frames]
    
#     return full_motion

def generate_long_sequence(cfg, model, diffusion, tokenizer, text_model, tsv_path):
    # 1. è§£æ TSV
    words = parse_tsv(tsv_path)
    if not words: return None
    
    # ğŸ”´ ä¿®æ­£ç‚¹ï¼šä¸è¦åªå–æœ€åä¸€ä¸ªï¼Œè¦æ‰¾æœ€å¤§å€¼ï¼
    # éå†æ‰€æœ‰è¯ï¼Œæ‰¾åˆ°æœ€å¤§çš„ end_time
    total_duration = max([w[2] for w in words])
    
    # æˆ–è€…æ‰‹åŠ¨å¼ºåˆ¶æŒ‡å®šæ—¶é•¿ï¼ˆå¦‚æœä½ ç¡®ä¿¡æ˜¯ 60 ç§’ï¼‰
    # total_duration = 60.0 
    
    FPS = 30 
    CLIP_FRAMES = 120 
    CLIP_DURATION = CLIP_FRAMES / FPS 
    num_clips = math.ceil(total_duration / CLIP_DURATION)
    
    print(f"ğŸ¯ çœŸå®æ—¶é•¿: {total_duration:.2f}s | è®¡åˆ’ç”Ÿæˆ: {num_clips} æ®µ")
    
    generated_clips = []
    
    for i in range(num_clips):
        start_t = i * CLIP_DURATION
        end_t = start_t + CLIP_DURATION
        window_text = get_words_in_window(words, start_t, end_t)
        print(f"   - Clip {i+1}/{num_clips} [{start_t:.1f}s-{end_t:.1f}s]: '{window_text}'")
        
        text_emb = get_text_embedding(window_text, tokenizer, text_model, device)
        # æ³¨æ„ï¼šè¿™é‡Œå¤§å°å†™è¦å’Œ Config ç±»ä¸€è‡´ï¼Œä¹‹å‰æ˜¯ input_feats
        sample_shape = (1, CLIP_FRAMES, cfg.INPUT_FEATS) 
        
        # ç”Ÿæˆ
        raw_motion = diffusion.sample(sample_shape, text_emb, src_mask=None, guidance_scale=2.5)
        raw_motion = raw_motion.squeeze(0).cpu().numpy()
        generated_clips.append(raw_motion)
        
    full_motion = np.vstack(generated_clips)
    
    # è®¡ç®—ç›®æ ‡å¸§æ•°
    target_frames = int(total_duration * FPS)
    
    # æˆªå–
    if full_motion.shape[0] > target_frames:
        print(f"âœ‚ï¸  è£å‰ªå¤šä½™å¸§æ•°: {full_motion.shape[0]} -> {target_frames}")
        full_motion = full_motion[:target_frames]
    
    return full_motion


def main():
    # ==============================================================
    # 1. Configuration (Modify paths here)
    # ==============================================================
    input_dir = "/home/hti_2025/hti_2025/dataset/genea2023_dataset/trn/main-agent/tsv"
    output_dir = "/home/hti_2025/wei/mywork/results/trn/main-agent"
    
    # Reference BVH for skeleton structure
    ref_bvh_path = "/home/hti_2025/hti_2025/dataset/genea2023_dataset/val/interloctr/bvh/val_2023_v0_000_interloctr.bvh"
    
    # Model Checkpoint
    checkpoint_path = "/home/hti_2025/wei/mywork/checkpoints/model_epoch_600.pt"

    # Load Config
    cfg = Config()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ==============================================================
    # 2. Initialize Models
    # ==============================================================
    print(f"ğŸš€ Initializing models on device: {device}...")
    
    # Load BERT
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    text_model = AutoModel.from_pretrained("bert-base-uncased").to(device).eval()
    
    # Load Diffusion Model
    model = Text2GestureModel(
        input_feats=cfg.INPUT_FEATS, latent_dim=cfg.LATENT_DIM, 
        n_heads=cfg.HEADS, n_layers=cfg.LAYERS, text_dim=cfg.TEXT_DIM
    ).to(device)
    
    diffusion = GaussianDiffusion(
        model=model, timesteps=cfg.DIFFUSION_STEPS, 
        loss_type=cfg.LOSS_TYPE, beta_schedule=cfg.BETA_SCHEDULE
    ).to(device)
    
    # Load Weights
    print(f"â™»ï¸ Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if 'model_state_dict' in checkpoint:
        diffusion.load_state_dict(checkpoint['model_state_dict'])
        data_mean = checkpoint.get('data_mean', np.array(cfg.DATA_MEAN))
        data_std = checkpoint.get('data_std', np.array(cfg.DATA_STD))
    else:
        diffusion.load_state_dict(checkpoint)
        data_mean = np.array(cfg.DATA_MEAN)
        data_std = np.array(cfg.DATA_STD)
        
    diffusion.eval()

    # Pre-process Mean/Std to CPU Numpy
    if isinstance(data_mean, torch.Tensor): data_mean = data_mean.cpu().numpy()
    if isinstance(data_std, torch.Tensor): data_std = data_std.cpu().numpy()
    
    # Avoid division by zero or extreme scaling
    data_std = np.clip(data_std, a_min=1e-6, a_max=None)

    # ==============================================================
    # 3. Batch Processing Loop
    # ==============================================================
    os.makedirs(output_dir, exist_ok=True)
    tsv_files = sorted(glob.glob(os.path.join(input_dir, "*.tsv")))
    
    print(f"\nğŸ“‚ Found {len(tsv_files)} TSV files.")
    print(f"   Output Directory: {output_dir}\n")
    
    # Use tqdm for progress bar
    for tsv_path in tqdm(tsv_files, desc="Processing"):
        filename = os.path.basename(tsv_path)
        file_id = os.path.splitext(filename)[0]
        output_bvh_path = os.path.join(output_dir, f"{file_id}.bvh")
        
        try:
            # --- A. Generate Motion ---
            full_raw_motion = generate_long_sequence(cfg, model, diffusion, tokenizer, text_model, tsv_path)
            
            if full_raw_motion is None:
                # print(f"âš ï¸ Skipped {filename}: Generation failed.")
                continue

            # --- B. Post-Processing ---
            
            # 1. To CPU Numpy (Safe conversion)
            if isinstance(full_raw_motion, torch.Tensor):
                raw_np = full_raw_motion.cpu().numpy()
            else:
                raw_np = full_raw_motion

            # 2. Denormalize
            denorm_motion = raw_np * data_std + data_mean

            # 3. Gaussian Smoothing (Sigma=9.0 as requested)
            denorm_motion = gaussian_filter1d(denorm_motion, sigma=9.0, axis=0)

            # 4. SVD Cleanup (Auto-reshapes 2D -> 3D inside)
            clean_motion = cleanup_12d_data(denorm_motion)
            
            # --- C. Export BVH ---
            export_bvh_manual(clean_motion, ref_bvh_path, output_bvh_path)

        except Exception as e:
            print(f"\nâŒ [Error] Failed to process {filename}: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\nğŸ‰ Batch processing finished!")

if __name__ == '__main__':
    main()
